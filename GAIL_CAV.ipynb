{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FIi8t8NUTEJ"
   },
   "source": [
    "# Human driving - [GAIL](https://arxiv.org/abs/1606.03476)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:36:55.624114Z",
     "start_time": "2021-07-17T17:36:55.042240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/bkarkada3/lane-change-gail\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Utils.Environment_LC import ENVIRONMENT\n",
    "from Utils.PPO import PPO\n",
    "from Utils.GAIL import DISCRIMINATOR_FUNCTION   # NEW compare to PPO code\n",
    "\n",
    "Path = os.getcwd()\n",
    "print(Path)\n",
    "PATH = \"Trained_model/GAIL_0.pth\"\n",
    "\n",
    "#CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Training\n",
    "try:\n",
    "    expert_traj = np.load(\"Expert_trajectory/expert_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False\n",
    "\n",
    "# Testing\n",
    "try:\n",
    "    testing_traj = np.load(\"Expert_trajectory/testing_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIDM_do = True\n",
    "A_para = 'normal'\n",
    "B_para = '2000'\n",
    "Env = ENVIRONMENT(\n",
    "    para_B                          = B_para,                               \n",
    "    para_A                          = A_para, \n",
    "    noise                           = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:37:00.215453Z",
     "start_time": "2021-07-17T17:36:59.044591Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "actor.0.weight \t torch.Size([64, 11])\n",
      "actor.0.bias \t torch.Size([64])\n",
      "actor.2.weight \t torch.Size([64, 64])\n",
      "actor.2.bias \t torch.Size([64])\n",
      "actor.4.weight \t torch.Size([2, 64])\n",
      "actor.4.bias \t torch.Size([2])\n",
      "critic.0.weight \t torch.Size([64, 11])\n",
      "critic.0.bias \t torch.Size([64])\n",
      "critic.2.weight \t torch.Size([64, 64])\n",
      "critic.2.bias \t torch.Size([64])\n",
      "critic.4.weight \t torch.Size([1, 64])\n",
      "critic.4.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "K_epochs = 16               # update policy for K epochs\n",
    "eps_clip = 0.25             # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.001#0.001            # learning rate for actor network\n",
    "lr_critic = 0.001#0.001           # learning rate for critic network\n",
    "\n",
    "has_continuous_action_space = True\n",
    "\n",
    "update_episode = 2#8\n",
    "\n",
    "total_episodes   = 400\n",
    "# action_std_decay_freq = 10\n",
    "# action_std_decay_rate = 0.01\n",
    "# min_action_std = 0.01  \n",
    "save_model_freq = 1\n",
    "\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 2 \n",
    "action_bound = 2\n",
    "\n",
    "ppo_agent  = PPO(state_dim, action_dim, action_bound, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.4)\n",
    "\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in ppo_agent.policy.state_dict():\n",
    "    print(param_tensor, \"\\t\", ppo_agent.policy.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Discriminator\n",
    "D_epochs    = 16     # update discriminator for D epochs\n",
    "\n",
    "lr_gail = 0.0001\n",
    "\n",
    "expert_sample_size = expert_traj.shape[0] #len(expert_traj) \n",
    "\n",
    "Discriminator = DISCRIMINATOR_FUNCTION(state_dim, action_dim, lr_gail, D_epochs, expert_traj, expert_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Starting Episode 1 / 400\n",
      "Driver type: normal, Iteration: 1 / 3\n",
      "Driver type: aggressive, Iteration: 2 / 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 2)) of distribution MultivariateNormal(loc: torch.Size([1, 2]), covariance_matrix: torch.Size([1, 2, 2])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m Env\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m---> 74\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mppo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     Env\u001b[38;5;241m.\u001b[39mrun(action) \u001b[38;5;66;03m# run human behavior\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     state_next, _ \u001b[38;5;241m=\u001b[39m Env\u001b[38;5;241m.\u001b[39mobserve()\n",
      "File \u001b[0;32m~/lane-change-gail/Utils/PPO.py:89\u001b[0m, in \u001b[0;36mPPO.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     88\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 89\u001b[0m     action, action_logprob, state_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_old\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/lane-change-gail/Utils/NeuralNetwork.py:64\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m     cov_mat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_var)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m#print(\"Covariance Matrix:\", cov_mat)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m    176\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 2)) of distribution MultivariateNormal(loc: torch.Size([1, 2]), covariance_matrix: torch.Size([1, 2, 2])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "LC_end_pos = 0.5      # lateral position deviation\n",
    "LC_end_yaw = 0.005    # yaw angle deviation \n",
    "PARAS = ['aggressive', 'normal', 'cautious']\n",
    "\n",
    "path_idx    = 0\n",
    "Time_len = 800\n",
    "lane_wid = 3.75               \n",
    "veh_len  = 5.0\n",
    "v_0      = 30   \n",
    "\n",
    "print(\"Training started...\")\n",
    "\n",
    "#for episode in tqdm(range(total_episodes)):\n",
    "for episode in range(total_episodes):\n",
    "    print(f\"Starting Episode {episode + 1} / {total_episodes}\")\n",
    "    \n",
    "    # Collect PPO outputs\n",
    "    states      = [] # NEW compare to PPO code\n",
    "    actions     = [] # NEW compare to PPO code\n",
    "    \n",
    "    for iter in range(3):\n",
    "        print(f\"Driver type: {A_para}, Iteration: {iter + 1} / 3\")\n",
    "\n",
    "        A_para = PARAS[iter]   # interact with different driver types iteratively, aggressive, normal, and cautious\n",
    "    \n",
    "        # Environment    \n",
    "        Env.reset()   \n",
    "        LC_start = False   \n",
    "        LC_starttime = 0\n",
    "        LC_endtime   = 0\n",
    "        LC_mid       = 0\n",
    "\n",
    "        for t in range(1, Time_len):  \n",
    "            s_t, env_t = Env.observe()       #Observation        \n",
    "            if t != env_t + 1:               # check time consistency between Env and simulation codee\n",
    "                print('warning: time inconsistency!')\n",
    "\n",
    "            Dat = Env.read()                 # Read ground-truth information\n",
    "\n",
    "            #Lane change indication\n",
    "            if Dat[t-1,24]!=0 and LC_start == False and LC_starttime == 0:                 # if LC is true at the end of last time step\n",
    "                LC_start = True  \n",
    "                LC_starttime = t\n",
    "            # finish lane change - stop in the center of the target lane\n",
    "            elif abs(Dat[t-1,25] - 0.5*lane_wid) <= LC_end_pos and abs(Dat[t-1,26]) <= LC_end_yaw and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "            # out of boundary\n",
    "            elif (Dat[t-1,25] <= - lane_wid or Dat[t-1,25] > 2.0 * lane_wid) and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "            \n",
    "            # B cross the line    \n",
    "            if Dat[t-1,25]<=lane_wid and LC_mid==0:\n",
    "                LC_mid = t         # record the time cross lane-marking\n",
    "\n",
    "            # Low-level task: action              \n",
    "            if LC_start == False:\n",
    "                # longitudinal\n",
    "                if Dat[t-1,25] > lane_wid:    # B in lane 2, B follwo F\n",
    "                    act_0 = Env.IDM_B(Dat[t-1,13], Dat[t-1,13] - Dat[t-1,10], Dat[t-1,9] - Dat[t-1,12] - veh_len) #IDM\n",
    "                elif Dat[t-1,25] <= lane_wid:   # B cross the line, B follow E\n",
    "                    act_0 = Env.IDM_B(Dat[t-1,13], Dat[t-1,13] - Dat[t-1,1], Dat[t-1,0] - Dat[t-1,12] - veh_len) #IDM\n",
    "                \n",
    "                # lateral\n",
    "                act_1 = 0                   # yaw rate\n",
    "                \n",
    "                action = [act_0, act_1]\n",
    "                Env.run(action)\n",
    "            \n",
    "            else:\n",
    "                state, _ = Env.observe()\n",
    "\n",
    "                action = ppo_agent.select_action(state)\n",
    "                Env.run(action) # run human behavior\n",
    "                state_next, _ = Env.observe()\n",
    "\n",
    "                reward = Discriminator.reward(state, action)    # reward from the discriminator    # NEW compare to PPO code        \n",
    "                \n",
    "                if t == Time_len - 1:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "                    \n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "\n",
    "                # Collect PPO outputs\n",
    "                states.append(state) # NEW compare to PPO code\n",
    "                actions.append(action) # NEW compare to PPO code\n",
    "\n",
    "        \n",
    "    # Policy\n",
    "    if episode % update_episode  == 0:\n",
    "        print(f\"Updating PPO policy at Episode {episode + 1}\")\n",
    "        # PPO\n",
    "        ppo_agent.update()\n",
    "\n",
    "        # Discriminator\n",
    "        states    = torch.FloatTensor(np.array(states)).squeeze().to(device) # NEW compare to PPO code\n",
    "        actions   = torch.FloatTensor(np.array(actions)).to(device) # NEW compare to PPO code\n",
    "        Discriminator.update(ppo_agent, states, actions) # NEW compare to PPO code\n",
    "\n",
    "    ############# Record episode results ##########\n",
    "    # write conditions you want to save the trained model during the training\n",
    "    # Code\n",
    "    \n",
    "    if episode % save_model_freq  == 0:\n",
    "        path_idx   += 1\n",
    "        max_score = -np.inf\n",
    "        PATH       = \"Trained_model/GAIL_\"+str(path_idx)+\".pth\"\n",
    "        print(f\"Saving model at Episode {episode + 1}, Path: {PATH}\")\n",
    "        ppo_agent.save(PATH)\n",
    "        \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Time Series Prediction with LSTM Using PyTorch",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.488px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "512.4px",
    "left": "643.8px",
    "right": "20px",
    "top": "26px",
    "width": "682px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
