{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FIi8t8NUTEJ"
   },
   "source": [
    "# Human driving - [GAIL](https://arxiv.org/abs/1606.03476)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:36:55.624114Z",
     "start_time": "2021-07-17T17:36:55.042240Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Environment_LC.py, line 382)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3526\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 9\u001b[0;36m\n\u001b[0;31m    from Utils.Environment_LC import ENVIRONMENT\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/lane-change-gail/Utils/Environment_LC.py:382\u001b[0;36m\u001b[0m\n\u001b[0;31m    if self.A[self.t+1,1] < 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Utils.Environment_LC import ENVIRONMENT\n",
    "from Utils.PPO import PPO\n",
    "from Utils.GAIL import DISCRIMINATOR_FUNCTION   # NEW compare to PPO code\n",
    "\n",
    "Path = os.getcwd()\n",
    "print(Path)\n",
    "PATH = \"Trained_model/GAIL_0.pth\"\n",
    "\n",
    "#CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Training\n",
    "try:\n",
    "    expert_traj = np.load(\"Expert_trajectory/expert_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False\n",
    "\n",
    "# Testing\n",
    "try:\n",
    "    testing_traj = np.load(\"Expert_trajectory/testing_traj.npy\", allow_pickle=True)\n",
    "except:\n",
    "    print(\"Train, generate and save expert trajectories\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIDM_do = True\n",
    "A_para = 'normal'\n",
    "B_para = '2000'\n",
    "Env = ENVIRONMENT(\n",
    "    para_B                          = B_para,                               \n",
    "    para_A                          = A_para, \n",
    "    noise                           = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T17:37:00.215453Z",
     "start_time": "2021-07-17T17:36:59.044591Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_epochs = 16               # update policy for K epochs\n",
    "eps_clip = 0.25             # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.001#0.001            # learning rate for actor network\n",
    "lr_critic = 0.001#0.001           # learning rate for critic network\n",
    "\n",
    "has_continuous_action_space = True\n",
    "\n",
    "update_episode = 2#8\n",
    "\n",
    "total_episodes   = 400\n",
    "# action_std_decay_freq = 10\n",
    "# action_std_decay_rate = 0.01\n",
    "# min_action_std = 0.01  \n",
    "save_model_freq = 1\n",
    "\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 2 \n",
    "action_bound = 2\n",
    "\n",
    "ppo_agent  = PPO(state_dim, action_dim, action_bound, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.4)\n",
    "\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in ppo_agent.policy.state_dict():\n",
    "    print(param_tensor, \"\\t\", ppo_agent.policy.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW compare to PPO code\n",
    "# Discriminator\n",
    "D_epochs    = 16     # update discriminator for D epochs\n",
    "\n",
    "lr_gail = 0.0001\n",
    "\n",
    "expert_sample_size = expert_traj.shape[0] #len(expert_traj) \n",
    "\n",
    "Discriminator = DISCRIMINATOR_FUNCTION(state_dim, action_dim, lr_gail, D_epochs, expert_traj, expert_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LC_end_pos = 0.5      # lateral position deviation\n",
    "LC_end_yaw = 0.005    # yaw angle deviation \n",
    "PARAS = ['aggressive', 'normal', 'cautious']\n",
    "\n",
    "path_idx    = 0\n",
    "Time_len = 800\n",
    "lane_wid = 3.75               \n",
    "veh_len  = 5.0\n",
    "v_0      = 30   \n",
    "\n",
    "print(\"Training started...\")\n",
    "\n",
    "#for episode in tqdm(range(total_episodes)):\n",
    "for episode in range(total_episodes):\n",
    "    print(f\"Starting Episode {episode + 1} / {total_episodes}\")\n",
    "    \n",
    "    # Collect PPO outputs\n",
    "    states      = [] # NEW compare to PPO code\n",
    "    actions     = [] # NEW compare to PPO code\n",
    "    \n",
    "    for iter in range(3):\n",
    "        print(f\"Driver type: {A_para}, Iteration: {iter + 1} / 3\")\n",
    "\n",
    "        A_para = PARAS[iter]   # interact with different driver types iteratively, aggressive, normal, and cautious\n",
    "    \n",
    "        # Environment    \n",
    "        Env.reset()   \n",
    "        LC_start = False   \n",
    "        LC_starttime = 0\n",
    "        LC_endtime   = 0\n",
    "        LC_mid       = 0\n",
    "\n",
    "        for t in range(1, Time_len):  \n",
    "            s_t, env_t = Env.observe()       #Observation        \n",
    "            if t != env_t + 1:               # check time consistency between Env and simulation codee\n",
    "                print('warning: time inconsistency!')\n",
    "\n",
    "            Dat = Env.read()                 # Read ground-truth information\n",
    "\n",
    "            #Lane change indication\n",
    "            if Dat[t-1,24]!=0 and LC_start == False and LC_starttime == 0:                 # if LC is true at the end of last time step\n",
    "                LC_start = True  \n",
    "                LC_starttime = t\n",
    "            # finish lane change - stop in the center of the target lane\n",
    "            elif abs(Dat[t-1,25] - 0.5*lane_wid) <= LC_end_pos and abs(Dat[t-1,26]) <= LC_end_yaw and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "            # out of boundary\n",
    "            elif (Dat[t-1,25] <= - lane_wid or Dat[t-1,25] > 2.0 * lane_wid) and LC_start == True and LC_endtime == 0:       \n",
    "                LC_start = False\n",
    "                LC_endtime   = t\n",
    "            \n",
    "            # B cross the line    \n",
    "            if Dat[t-1,25]<=lane_wid and LC_mid==0:\n",
    "                LC_mid = t         # record the time cross lane-marking\n",
    "\n",
    "            # Low-level task: action              \n",
    "            if LC_start == False:\n",
    "                # longitudinal\n",
    "                if Dat[t-1,25] > lane_wid:    # B in lane 2, B follwo F\n",
    "                    act_0 = Env.IDM_B(Dat[t-1,13], Dat[t-1,13] - Dat[t-1,10], Dat[t-1,9] - Dat[t-1,12] - veh_len) #IDM\n",
    "                elif Dat[t-1,25] <= lane_wid:   # B cross the line, B follow E\n",
    "                    act_0 = Env.IDM_B(Dat[t-1,13], Dat[t-1,13] - Dat[t-1,1], Dat[t-1,0] - Dat[t-1,12] - veh_len) #IDM\n",
    "                \n",
    "                # lateral\n",
    "                act_1 = 0                   # yaw rate\n",
    "                \n",
    "                action = [act_0, act_1]\n",
    "                Env.run(action)\n",
    "            \n",
    "            else:\n",
    "                state, _ = Env.observe()\n",
    "\n",
    "                action = ppo_agent.select_action(state)\n",
    "                Env.run(action) # run human behavior\n",
    "                state_next, _ = Env.observe()\n",
    "\n",
    "                reward = Discriminator.reward(state, action)    # reward from the discriminator    # NEW compare to PPO code        \n",
    "                \n",
    "                if t == Time_len - 1:\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "                    \n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "\n",
    "                # Collect PPO outputs\n",
    "                states.append(state) # NEW compare to PPO code\n",
    "                actions.append(action) # NEW compare to PPO code\n",
    "\n",
    "        \n",
    "    # Policy\n",
    "    if episode % update_episode  == 0:\n",
    "        print(f\"Updating PPO policy at Episode {episode + 1}\")\n",
    "        # PPO\n",
    "        ppo_agent.update()\n",
    "\n",
    "        # Discriminator\n",
    "        states    = torch.FloatTensor(np.array(states)).squeeze().to(device) # NEW compare to PPO code\n",
    "        actions   = torch.FloatTensor(np.array(actions)).to(device) # NEW compare to PPO code\n",
    "        Discriminator.update(ppo_agent, states, actions) # NEW compare to PPO code\n",
    "\n",
    "    ############# Record episode results ##########\n",
    "    # write conditions you want to save the trained model during the training\n",
    "    # Code\n",
    "    \n",
    "    if episode % save_model_freq  == 0:\n",
    "        path_idx   += 1\n",
    "        max_score = -np.inf\n",
    "        PATH       = \"Trained_model/GAIL_\"+str(path_idx)+\".pth\"\n",
    "        print(f\"Saving model at Episode {episode + 1}, Path: {PATH}\")\n",
    "        ppo_agent.save(PATH)\n",
    "        \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Time Series Prediction with LSTM Using PyTorch",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.488px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "512.4px",
    "left": "643.8px",
    "right": "20px",
    "top": "26px",
    "width": "682px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
